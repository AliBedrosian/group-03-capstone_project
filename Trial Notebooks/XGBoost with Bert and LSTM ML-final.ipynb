{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a1cf98-cc07-4ed4-85e2-ea1cf8d493e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4709 entries, 0 to 4708\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   tweet_id            4709 non-null   int64  \n",
      " 1   date                4709 non-null   object \n",
      " 2   tweet_body          4709 non-null   object \n",
      " 3   roberta_sentiment   4709 non-null   object \n",
      " 4   roberta_pos_score   4709 non-null   float64\n",
      " 5   roberta_neg_score   4709 non-null   float64\n",
      " 6   roberta_neu_score   4709 non-null   float64\n",
      " 7   sentiment_polarity  4709 non-null   float64\n",
      " 8   like_count          4709 non-null   int64  \n",
      " 9   retweet_count       4709 non-null   int64  \n",
      " 10  engagement_score    4709 non-null   int64  \n",
      " 11  Open                4709 non-null   float64\n",
      " 12  Close               4709 non-null   float64\n",
      " 13  pct_change          4709 non-null   float64\n",
      "dtypes: float64(7), int64(4), object(3)\n",
      "memory usage: 515.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_body</th>\n",
       "      <th>roberta_sentiment</th>\n",
       "      <th>roberta_pos_score</th>\n",
       "      <th>roberta_neg_score</th>\n",
       "      <th>roberta_neu_score</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>engagement_score</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>pct_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1655978502187778073</td>\n",
       "      <td>2023-05-09</td>\n",
       "      <td>Yup</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.290412</td>\n",
       "      <td>0.215288</td>\n",
       "      <td>0.494300</td>\n",
       "      <td>0.075124</td>\n",
       "      <td>39533</td>\n",
       "      <td>3255</td>\n",
       "      <td>42788</td>\n",
       "      <td>168.949997</td>\n",
       "      <td>169.149994</td>\n",
       "      <td>0.118376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1655968899903418373</td>\n",
       "      <td>2023-05-09</td>\n",
       "      <td>Massive public manipulation</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.723850</td>\n",
       "      <td>0.266601</td>\n",
       "      <td>-0.714301</td>\n",
       "      <td>49528</td>\n",
       "      <td>9811</td>\n",
       "      <td>59339</td>\n",
       "      <td>168.949997</td>\n",
       "      <td>169.149994</td>\n",
       "      <td>0.118376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1646228474628280326</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>ü§£ü§£</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.310404</td>\n",
       "      <td>0.235308</td>\n",
       "      <td>0.454288</td>\n",
       "      <td>0.075095</td>\n",
       "      <td>108462</td>\n",
       "      <td>10198</td>\n",
       "      <td>118660</td>\n",
       "      <td>190.740005</td>\n",
       "      <td>180.539993</td>\n",
       "      <td>-5.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1640171198091866114</td>\n",
       "      <td>2023-03-27</td>\n",
       "      <td>Prescient</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.204723</td>\n",
       "      <td>0.126566</td>\n",
       "      <td>0.668711</td>\n",
       "      <td>0.078156</td>\n",
       "      <td>56272</td>\n",
       "      <td>9193</td>\n",
       "      <td>65465</td>\n",
       "      <td>194.419998</td>\n",
       "      <td>191.809998</td>\n",
       "      <td>-1.342455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1742235895166652609</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>Congratulations Tesla team on a great year!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.991541</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.007122</td>\n",
       "      <td>0.990203</td>\n",
       "      <td>67751</td>\n",
       "      <td>5222</td>\n",
       "      <td>72973</td>\n",
       "      <td>250.080002</td>\n",
       "      <td>248.419998</td>\n",
       "      <td>-0.663789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id        date  \\\n",
       "0  1655978502187778073  2023-05-09   \n",
       "1  1655968899903418373  2023-05-09   \n",
       "2  1646228474628280326  2023-04-12   \n",
       "3  1640171198091866114  2023-03-27   \n",
       "4  1742235895166652609  2024-01-02   \n",
       "\n",
       "                                     tweet_body roberta_sentiment  \\\n",
       "0                                           Yup           neutral   \n",
       "1                   Massive public manipulation          negative   \n",
       "2                                            ü§£ü§£           neutral   \n",
       "3                                     Prescient           neutral   \n",
       "4  Congratulations Tesla team on a great year!!          positive   \n",
       "\n",
       "   roberta_pos_score  roberta_neg_score  roberta_neu_score  \\\n",
       "0           0.290412           0.215288           0.494300   \n",
       "1           0.009549           0.723850           0.266601   \n",
       "2           0.310404           0.235308           0.454288   \n",
       "3           0.204723           0.126566           0.668711   \n",
       "4           0.991541           0.001338           0.007122   \n",
       "\n",
       "   sentiment_polarity  like_count  retweet_count  engagement_score  \\\n",
       "0            0.075124       39533           3255             42788   \n",
       "1           -0.714301       49528           9811             59339   \n",
       "2            0.075095      108462          10198            118660   \n",
       "3            0.078156       56272           9193             65465   \n",
       "4            0.990203       67751           5222             72973   \n",
       "\n",
       "         Open       Close  pct_change  \n",
       "0  168.949997  169.149994    0.118376  \n",
       "1  168.949997  169.149994    0.118376  \n",
       "2  190.740005  180.539993   -5.347600  \n",
       "3  194.419998  191.809998   -1.342455  \n",
       "4  250.080002  248.419998   -0.663789  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"tableau_ready_sentiment_data.csv\")  # Update the path if needed\n",
    "\n",
    "# Show basic info and preview\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba0c9f3-e02c-481d-b8d1-1014a865aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Preprocess text and combine with numerical features\n",
    "\n",
    "# Drop rows with missing tweet text (just in case)\n",
    "df = df.dropna(subset=[\"tweet_body\"])\n",
    "\n",
    "# Convert tweet text into TF-IDF features (max 300 terms)\n",
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "X_text = vectorizer.fit_transform(df[\"tweet_body\"]).toarray()\n",
    "\n",
    "# Select numerical columns to include as features\n",
    "numerical_cols = [\n",
    "    \"roberta_pos_score\", \n",
    "    \"roberta_neg_score\", \n",
    "    \"roberta_neu_score\", \n",
    "    \"sentiment_polarity\", \n",
    "    \"engagement_score\"\n",
    "]\n",
    "X_num = df[numerical_cols].values\n",
    "\n",
    "# Normalize the numerical features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)\n",
    "\n",
    "# Combine text features and numerical features into one array\n",
    "X = np.hstack((X_text, X_num_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505dab66-0025-4b93-b544-b9de2f7244fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define target variables and split the dataset\n",
    "\n",
    "# Define target for classification: sentiment labels\n",
    "y_class = df[\"roberta_sentiment\"]\n",
    "\n",
    "# Define target for regression: percentage change in stock price\n",
    "y_reg = df[\"pct_change\"]\n",
    "\n",
    "# Split data for classification (stratify to preserve class balance)\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
    "    X, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "# Split data for regression (no stratify needed)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_reg, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e53d5ef-8a15-4179-be26-1f60b0792dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from xgboost) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Install XGBoost \n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d13821-55f0-49e4-bde5-34ff35d81326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - MAE: 2.4119\n",
      "XGBoost - RMSE: 3.0341\n",
      "XGBoost - R¬≤ Score: -0.0423\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Train and evaluate regression with XGBoost\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Initialize the XGBoost model with 100 trees and a learning rate of 0.1\n",
    "xgb_reg = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model on the regression training data\n",
    "xgb_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_xgb = xgb_reg.predict(X_test_reg)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae_xgb = mean_absolute_error(y_test_reg, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test_reg, y_pred_xgb))\n",
    "r2_xgb = r2_score(y_test_reg, y_pred_xgb)\n",
    "\n",
    "# Display results\n",
    "print(f\"XGBoost - MAE: {mae_xgb:.4f}\")\n",
    "print(f\"XGBoost - RMSE: {rmse_xgb:.4f}\")\n",
    "print(f\"XGBoost - R¬≤ Score: {r2_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2ad010-4ea6-4183-b907-3b86f8e74a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Feature engineering to improve regression performance\n",
    "\n",
    "# Feature 1: Tweet length (number of characters)\n",
    "df[\"tweet_length\"] = df[\"tweet_body\"].apply(len)\n",
    "\n",
    "# Feature 2: Day of the week the tweet was posted (0 = Monday, 6 = Sunday)\n",
    "df[\"day_of_week\"] = pd.to_datetime(df[\"date\"]).dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "\n",
    "# Feature 3: One-hot encode the categorical sentiment labels\n",
    "sentiment_dummies = pd.get_dummies(df[\"roberta_sentiment\"], prefix=\"sent\")\n",
    "\n",
    "# Feature 4: Interaction features (engagement * sentiment score)\n",
    "df[\"eng_pos\"] = df[\"engagement_score\"] * df[\"roberta_pos_score\"]\n",
    "df[\"eng_neg\"] = df[\"engagement_score\"] * df[\"roberta_neg_score\"]\n",
    "df[\"eng_neu\"] = df[\"engagement_score\"] * df[\"roberta_neu_score\"]\n",
    "\n",
    "# Combine all numeric features into one DataFrame\n",
    "feature_cols = [\n",
    "    \"roberta_pos_score\", \"roberta_neg_score\", \"roberta_neu_score\",\n",
    "    \"sentiment_polarity\", \"engagement_score\",\n",
    "    \"tweet_length\", \"day_of_week\",\n",
    "    \"eng_pos\", \"eng_neg\", \"eng_neu\"\n",
    "]\n",
    "\n",
    "X_new = pd.concat([df[feature_cols], sentiment_dummies], axis=1)\n",
    "\n",
    "# Standardize the features to improve model performance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_new)\n",
    "\n",
    "# Define regression target again\n",
    "y_reg = df[\"pct_change\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d49e475-7de7-4e73-bd64-6b8c516e79f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved XGBoost - MAE: 2.4245\n",
      "Improved XGBoost - RMSE: 3.0376\n",
      "Improved XGBoost - R¬≤ Score: -0.0447\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Re-train XGBoost regression with new features\n",
    "\n",
    "# Split engineered feature matrix and target into training and test sets\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "    X_scaled, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train a new XGBoost regression model\n",
    "xgb_reg2 = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_reg2.fit(X_train_new, y_train_new)\n",
    "\n",
    "# Predict stock price change on the test set\n",
    "y_pred_new = xgb_reg2.predict(X_test_new)\n",
    "\n",
    "# Evaluate model performance using common regression metrics\n",
    "mae_new = mean_absolute_error(y_test_new, y_pred_new)\n",
    "rmse_new = np.sqrt(mean_squared_error(y_test_new, y_pred_new))\n",
    "r2_new = r2_score(y_test_new, y_pred_new)\n",
    "\n",
    "# Show results\n",
    "print(f\"Improved XGBoost - MAE: {mae_new:.4f}\")\n",
    "print(f\"Improved XGBoost - RMSE: {rmse_new:.4f}\")\n",
    "print(f\"Improved XGBoost - R¬≤ Score: {r2_new:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9309598-00bc-4d59-aaa2-b4b1ccee0adc",
   "metadata": {},
   "source": [
    "Conclusion (so far)\n",
    "Even with better features:\n",
    "\n",
    "The model still isn't capturing a meaningful signal from the tweets to predict % stock change.\n",
    "\n",
    "Tweets alone may not be strong predictors without market context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb633320-37a6-4d29-b15c-02c7c4941912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4709/4709 [03:20<00:00, 23.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Generate BERT embeddings for tweet text\n",
    "\n",
    "!pip install transformers torch --quiet\n",
    "\n",
    "# Import BERT model and tokenizer from HuggingFace Transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tqdm import tqdm # To show progress bar\n",
    "\n",
    "# Load pretrained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert.eval()\n",
    "\n",
    "# Define function to generate mean-pooled BERT embeddings for a single tweet\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert(**inputs)\n",
    "    # Return the average of all token embeddings for this tweet\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Generate embeddings for every tweet in the dataset (can take time)\n",
    "tweet_texts = df[\"tweet_body\"].tolist()\n",
    "bert_embeddings = [get_bert_embedding(t) for t in tqdm(tweet_texts)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cafe12cc-615f-4851-b0bb-fa8bc01e7fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4709, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9: Check embedding shape\n",
    "# Convert list of embeddings to a NumPy array for easier processing\n",
    "bert_embeddings = np.array(bert_embeddings)\n",
    "bert_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f572e79a-c72e-420d-8910-4190732ca3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Combine BERT embeddings with engineered numeric features\n",
    "\n",
    "# Use the same engineered numeric features from earlier\n",
    "feature_cols = [\n",
    "    \"roberta_pos_score\", \"roberta_neg_score\", \"roberta_neu_score\",\n",
    "    \"sentiment_polarity\", \"engagement_score\",\n",
    "    \"tweet_length\", \"day_of_week\",\n",
    "    \"eng_pos\", \"eng_neg\", \"eng_neu\"\n",
    "]\n",
    "\n",
    "# One-hot encode sentiment again\n",
    "sentiment_dummies = pd.get_dummies(df[\"roberta_sentiment\"], prefix=\"sent\")\n",
    "\n",
    "# Concatenate all numeric features\n",
    "X_numeric = pd.concat([df[feature_cols], sentiment_dummies], axis=1)\n",
    "\n",
    "# Standardize numeric features for consistency\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# Concatenate BERT embeddings with scaled numeric features\n",
    "X_final = np.hstack((bert_embeddings, X_numeric_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9d1c21-56fb-4cfe-83b0-83694ef81ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (BERT + Features) - MAE: 2.3975\n",
      "XGBoost (BERT + Features) - RMSE: 3.0500\n",
      "XGBoost (BERT + Features) - R¬≤ Score: -0.0532\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Train XGBoost regression model using BERT + numeric features\n",
    "\n",
    "# Target variable\n",
    "y_final = df[\"pct_change\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train XGBoost model\n",
    "xgb_bert = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_bert.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_final = xgb_bert.predict(X_test_final)\n",
    "\n",
    "mae_bert = mean_absolute_error(y_test_final, y_pred_final)\n",
    "rmse_bert = np.sqrt(mean_squared_error(y_test_final, y_pred_final))\n",
    "r2_bert = r2_score(y_test_final, y_pred_final)\n",
    "\n",
    "# Results\n",
    "print(f\"XGBoost (BERT + Features) - MAE: {mae_bert:.4f}\")\n",
    "print(f\"XGBoost (BERT + Features) - RMSE: {rmse_bert:.4f}\")\n",
    "print(f\"XGBoost (BERT + Features) - R¬≤ Score: {r2_bert:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2ac065e-0809-4a73-ad13-9080dee9a734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (4.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\seven\\anaconda3\\envs\\dev\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Install TensorFlow (run once)\n",
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eacc31f-38ec-4b62-8241-8770393d6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Tokenize and pad tweet text for LSTM\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize tokenizer with a vocabulary size of 10,000 and an out-of-vocab token\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "\n",
    "# Fit the tokenizer on tweet text\n",
    "tokenizer.fit_on_texts(df[\"tweet_body\"])\n",
    "\n",
    "# Convert the tweets into sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(df[\"tweet_body\"])\n",
    "\n",
    "# Pad/truncate all sequences to the same length for input into LSTM\n",
    "max_length = 50\n",
    "X_seq = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Define the regression target: percent change in Tesla stock price\n",
    "y_seq = df[\"pct_change\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2f1ae04-8c3c-48f9-9aea-525b71658f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 9.2240 - val_loss: 8.1065\n",
      "Epoch 2/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.1766 - val_loss: 8.1093\n",
      "Epoch 3/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 9.1391 - val_loss: 8.1158\n",
      "Epoch 4/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.9491 - val_loss: 8.1345\n",
      "Epoch 5/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.0219 - val_loss: 8.1042\n",
      "Epoch 6/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.9912 - val_loss: 8.1064\n",
      "Epoch 7/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.3680 - val_loss: 8.1993\n",
      "Epoch 8/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 8.8217 - val_loss: 8.1172\n",
      "Epoch 9/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 9.2541 - val_loss: 8.1065\n",
      "Epoch 10/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 9.5217 - val_loss: 8.1057\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Define and train LSTM model for regression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Split the padded sequences and target for training/testing\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64),  # Embedding layer to learn word vectors\n",
    "    LSTM(64, return_sequences=False),           # LSTM layer for sequence processing\n",
    "    Dropout(0.3),                               # Dropout to reduce overfitting\n",
    "    Dense(32, activation='relu'),               # Fully connected hidden layer\n",
    "    Dropout(0.2),                               # Another dropout layer\n",
    "    Dense(1)                                    # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model using mean squared error loss and Adam optimizer\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model for 10 epochs, using 10% of training data for validation\n",
    "history = model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32,\n",
    "                    validation_split=0.1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a43dfe1c-977d-4da4-a99e-7f041728856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "LSTM - MAE: 2.3459\n",
      "LSTM - RMSE: 2.9728\n",
      "LSTM - R¬≤ Score: -0.0006\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Evaluate LSTM model performance\n",
    "\n",
    "# Make predictions on the test set using the trained LSTM model\n",
    "y_pred_lstm = model.predict(X_test_lstm).flatten()\n",
    "\n",
    "# Compute regression metrics for performance evaluation\n",
    "mae_lstm = mean_absolute_error(y_test_lstm, y_pred_lstm)\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test_lstm, y_pred_lstm))\n",
    "r2_lstm = r2_score(y_test_lstm, y_pred_lstm)\n",
    "\n",
    "# Show results\n",
    "print(f\"LSTM - MAE: {mae_lstm:.4f}\")\n",
    "print(f\"LSTM - RMSE: {rmse_lstm:.4f}\")\n",
    "print(f\"LSTM - R¬≤ Score: {r2_lstm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b94014a-ba23-4410-9b8b-1128a3aaa099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "pkl.dump(model, open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e09a3e5-43e6-429e-b2b5-ea11a2328835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
